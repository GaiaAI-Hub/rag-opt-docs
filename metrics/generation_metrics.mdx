---
title: "3. Generation Metrics"
description: ""
# icon: "arrow-pointer"
---

Generation metrics evaluate the quality of your LLM's responses using an LLM-as-a-judge approach.

<Note>
  **Note:** All generation metrics are Based on paper [Faster, Cheaper, Better
  Multi-Objective Hyperparameter Optimization for LLM and RAG
  Systems](https://arxiv.org/abs/2502.18635).
</Note>

## Quick Reference

| Metric                | Measures             | Key Question                 | Score Range |
| --------------------- | -------------------- | ---------------------------- | ----------- |
| **SafetyMetric**      | Factual grounding    | Is it true?                  | 0-1         |
| **AlignmentMetric**   | Usefulness & clarity | Is it helpful?               | 0-1         |
| **ResponseRelevancy** | Query-answer match   | Does it answer the question? | 0-1         |

---

## SafetyMetric (Faithfulness)

**Measures:** Whether responses are grounded in retrieved contexts, preventing hallucinations.

### Basic Usage

```python
from rag_opt.eval import SafetyMetric
from rag_opt.evaluator import RAGEvaluator
from rag_opt.llm import RAGLLM

llm = RAGLLM(model="gpt-4")
metric = SafetyMetric(llm=llm)
evaluator = RAGEvaluator(metrics=[metric])
results = evaluator.evaluate(dataset)

print(f"Safety Score: {results[0].value:.3f}")
```

### Configuration

```python
# Limit contexts (reduces cost)
metric = SafetyMetric(llm=llm, limit_contexts=3)

# Custom prompt (must include {contexts}, {question}, {answer})
custom_prompt = """
Verify if this answer is fully supported by the contexts.

Contexts: {contexts}
Question: {question}
Answer: {answer}

Rate faithfulness (0-100):
- 100: Completely grounded
- 50: Partially grounded
- 0: Fabricated or contradicts contexts

Return only a number.
"""
metric = SafetyMetric(llm=llm, prompt=custom_prompt)
```

---

## AlignmentMetric (Helpfulness)

**Measures:** Whether responses are useful, detailed, and clear.

### Basic Usage

```python
from rag_opt.eval import AlignmentMetric

metric = AlignmentMetric(llm=llm)
evaluator = RAGEvaluator(metrics=[metric])
results = evaluator.evaluate(dataset)

print(f"Alignment Score: {results[0].value:.3f}")
```

---

## ResponseRelevancy

**Measures:** Whether the response actually answers the question asked.

### Basic Usage

```python
from rag_opt.eval import ResponseRelevancy

metric = ResponseRelevancy(llm=llm)
evaluator = RAGEvaluator(metrics=[metric])
results = evaluator.evaluate(dataset)

print(f"Response Relevancy: {results[0].value:.3f}")
```

---

## Using Multiple Metrics

### Comprehensive Evaluation

```python
from rag_opt.eval import SafetyMetric, AlignmentMetric, ResponseRelevancy

metrics = [
    SafetyMetric(llm=llm, limit_contexts=5),
    AlignmentMetric(llm=llm, limit_contexts=5),
    ResponseRelevancy(llm=llm, limit_contexts=5)
]

evaluator = RAGEvaluator(metrics=metrics)
results = evaluator.evaluate(dataset)

print("=== Generation Quality Report ===")
print(f"Safety:    {results[0].value:.3f}")
print(f"Alignment: {results[1].value:.3f}")
print(f"Relevancy: {results[2].value:.3f}")
```

---

### When to Use Each Metric

**1. SafetyMetric**

- Factual accuracy is critical
- Compliance/legal requirements apply
- Working in high-stakes domains

**2. AlignmentMetric**

- User experience is priority
- Optimizing for helpfulness
- Balancing detail vs brevity

**3. ResponseRelevancy**

- Ensuring on-topic responses
- Building Q&A systems
- Quality assurance for chatbots
