---
title: Quickstart
description: ""
---

This quickstart guide walks you through setting up and running a basic RAG optimization pipeline. You’ll learn how to load dataset files, generate question–answer pairs, define a RAG configuration with tunable hyperparameters and models, and execute the optimization workflow.
You can also try it out directly in this [Notebook](https://colab.research.google.com/drive/1hrfAHCfm3x0Ov-amCEHpptMiyoqC-McE?usp=sharing)

<Steps>
<Step title="Install Requirements">

Install RAGOPT via pip:

```sh
pip install rag-opt
```

</Step>

<Step title="Prepare the RAG Configuration File">
Create a configuration file named `rag_config.yaml` in your project directory. This file defines the search space for your optimization.
You can also copy a sample configuration from the repository [RAG Parameters Config YAML File ](https://github.com/GaiaAI-Hub/rag-opt/blob/main/rag_config.yaml)

**Example:**

```yaml
# Minimal RAG Configuration Example

chunk_size:
  bounds: [512, 1024]
  dtype: int

max_tokens:
  bounds: [256, 512]
  dtype: int

chunk_overlap:
  bounds: [0, 200]
  dtype: int

temperature:
  bounds: [0.0, 1.0]
  dtype: float

vector_store:
  choices:
    faiss: {}
    pinecone:
      api_key: "YOUR_PINECONE_API_KEY"
      index_name: "your-index-name"

search_type:
  choices: ["similarity", "mmr"]

embedding:
  choices:
    huggingface:
      models:
        - "all-MiniLM-L6-v2"
    openai:
      api_key: "YOUR_OPENAI_API_KEY"
      models:
        - "text-embedding-ada-002"

k:
  bounds: [1, 10]
  dtype: int

use_reranker: false

llm:
  choices:
    openai:
      api_key: "YOUR_OPENAI_API_KEY"
      models:
        - "gpt-3.5-turbo"
    huggingface:
      models:
        - "gpt2-medium"
```

</Step>

<Step title="Generate the Dataset">

Generate a set of question–answer pairs to be used for optimizing the RAG pipeline.

You can use any LLM you prefer from [Available Chat Models](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html)

```python
from langchain.chat_models import init_chat_model
from rag_opt.rag import DatasetGenerator

# Initialize the language model
llm = init_chat_model(
    model="gpt-3.5-turbo",
    model_provider="openai",
    api_key="sk-***"
)

# Generate training data
data_gen = DatasetGenerator(llm,dataset_path="./data")
dataset = data_gen.generate(3)  # Generates 3 training samples
dataset.to_json("./rag_dataset.json")
```

</Step>

<Step title="Run Optimization">

Run the optimization process using your generated dataset and configuration file.

```python
from rag_opt.dataset import TrainDataset
from rag_opt.optimizer import Optimizer

# Load the training dataset
train_dataset = TrainDataset.from_json("rag_dataset.json")

# Initialize the optimizer
optimizer = Optimizer(
    train_dataset=train_dataset,
    config_path="rag_config.yaml",  # Path to your config file
    verbose=True
)

# Run optimization
best_config = optimizer.optimize(n_trials=3, best_one=True)  # Increase n_trials for better results
best_config.to_json()
```

</Step>

</Steps>

## Next Steps

- Explore the [Optimization Workflow](./workflow/introduction) for a deeper dive into the pipeline.
- Review [Metrics Overview](./metrics/overview) for advanced customization options.
- Build and experiment with your own RAG pipeline for production use.
